{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodological Foundation of a Numerical Taxonomy of Urban Form\n",
    "\n",
    "## Reproducible Python code to generate taxonomy\n",
    "\n",
    "Complete morphometrics assessment from input data to taxonomy.\n",
    "\n",
    "Input data:\n",
    " - building footprints\n",
    " - street network\n",
    " \n",
    "It is assumed that input data are cleaned to a required standard and stored in a single GeoPackage `files/geometry.gpkg` with two layers named `buildings` and `streets`. `buildings` area Polygons, whilst `streets` are LineStrings.\n",
    "\n",
    "Buildings data contain two attribute columns:\n",
    "\n",
    "- `sdbHei` is building height in meters.\n",
    "- `floor_area` is gross floor area (area * number of floors).\n",
    "\n",
    "This notebook requires `momepy` 0.3 or newer. The reproducible computational environment can be created using Docker container `darribas/gds_py:5.0`.\n",
    "\n",
    "The same code has been used to analyse both cases.\n",
    "\n",
    "### Generate additional morphometric elements\n",
    "\n",
    "Before we can start morhometrics we have to generate additional elements - tessellation and tessellation based blocks.\n",
    "\n",
    "First we import all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import momepy as mm\n",
    "import libpysal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import mapclassify\n",
    "\n",
    "from inequality.theil import Theil\n",
    "from tqdm import tqdm\n",
    "from momepy import limit_range\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load buildings and create unique ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"files/geometry.gpkg\"\n",
    "layer = \"buildings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings = gpd.read_file(path, layer=layer)\n",
    "buildings[\"uID\"] = range(len(buildings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morphological tessellation\n",
    "\n",
    "Check input for tessellation. If the input data is clean, the check will result in zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = mm.CheckTessellationInput(buildings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tessellation limited to 100 m buffer. Beware, it is memory demanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = mm.buffered_limit(buildings, 100)\n",
    "\n",
    "tess = mm.Tessellation(buildings, \"uID\", limit)\n",
    "tessellation = tess.tessellation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save tessellation to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessellation.to_file(\"files/geometry.gpkg\", layer=\"tessellation\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tessellation based blocks\n",
    "\n",
    "To generate tessellation based blocks we also need street network, therefore we need to read it to GeoDataFrame first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = gpd.read_file('files/geometry.gpkg', layer='streets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapped = mm.snap_street_network_edge(streets, buildings, 20, tessellation, 120, limit) # snap to close unwanted gaps\n",
    "blocks = mm.Blocks(tessellation, snapped, buildings, 'bID', 'uID')\n",
    "blocks_df = blocks.blocks  # get blocks df\n",
    "buildings['bID'] = blocks.buildings_id  # get block ID\n",
    "tessellation['bID'] = blocks.tessellation_id  # get block ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link streets\n",
    "\n",
    "We need to understand which building belongs to which street segment. We link IDs together based on proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets[\"nID\"] = range(len(streets))\n",
    "buildings['nID'] = mm.get_network_id(buildings, streets, 'nID', min_size=300)  # \n",
    "tessellation = tessellation.merge(buildings[['uID', 'nID']], on='uID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save elements to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'files/geometry.gpkg'\n",
    "tessellation.to_file(path, layer='tessellation', driver='GPKG')\n",
    "buildings.to_file(path, layer='buildings', driver='GPKG')\n",
    "blocks_df.to_file(path, layer='blocks', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure primary characters\n",
    "\n",
    "This part measures 74 primary morphometric characters.\n",
    "\n",
    "It does save intermediate parquet files as a backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess = tessellation\n",
    "blg = buildings\n",
    "blocks = blocks_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "- `sdbHei` is building height in meters. If you do not have it, skip affected lines.\n",
    "- `floor_area` is gross floor area (area * number of floors). If you do not have it, skip affected lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blg['sdbAre'] = mm.Area(blg).series\n",
    "blg['sdbVol'] = mm.Volume(blg, 'sdbHei', 'sdbAre').series\n",
    "blg['sdbPer'] = mm.Perimeter(blg).series\n",
    "blg['sdbCoA'] = mm.CourtyardArea(blg, 'sdbAre').series\n",
    "\n",
    "blg['ssbFoF'] = mm.FormFactor(blg, 'sdbVol', 'sdbAre').series\n",
    "blg['ssbVFR'] = mm.VolumeFacadeRatio(blg, 'sdbHei', 'sdbVol', 'sdbPer').series\n",
    "blg['ssbCCo'] = mm.CircularCompactness(blg, 'sdbAre').series\n",
    "blg['ssbCor'] = mm.Corners(blg).series\n",
    "blg['ssbSqu'] = mm.Squareness(blg).series\n",
    "blg['ssbERI'] = mm.EquivalentRectangularIndex(blg, 'sdbAre', 'sdbPer').series\n",
    "blg['ssbElo'] = mm.Elongation(blg).series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cencon = mm.CentroidCorners(blg)\n",
    "blg['ssbCCM'] = cencon.mean\n",
    "blg['ssbCCD'] = cencon.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blg['stbOri'] = mm.Orientation(blg).series\n",
    " \n",
    "tess['stcOri'] = mm.Orientation(tess).series\n",
    "blg['stbCeA'] = mm.CellAlignment(blg, tess, 'stbOri', 'stcOri', 'uID', 'uID').series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess['sdcLAL'] = mm.LongestAxisLength(tess).series\n",
    "tess['sdcAre'] = mm.Area(tess).series\n",
    "tess['sscCCo'] = mm.CircularCompactness(tess, 'sdcAre').series\n",
    "tess['sscERI'] = mm.EquivalentRectangularIndex(tess, 'sdcAre').series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blg[\"mtbSWR\"] = mm.SharedWallsRatio(blg, \"uID\", \"sdbPer\").series\n",
    " \n",
    "queen_1 = libpysal.weights.contiguity.Queen.from_dataframe(tess, ids=\"uID\")\n",
    " \n",
    "blg[\"mtbAli\"] = mm.Alignment(blg, queen_1, \"uID\", \"stbOri\").series\n",
    "blg[\"mtbNDi\"] = mm.NeighborDistance(blg, queen_1, \"uID\").series\n",
    "tess[\"mtcWNe\"] = mm.Neighbors(tess, queen_1, \"uID\", weighted=True).series\n",
    "tess[\"mdcAre\"] = mm.CoveredArea(tess, queen_1, \"uID\").series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blg_q1 = libpysal.weights.contiguity.Queen.from_dataframe(blg, silence_warnings=True)\n",
    " \n",
    "blg[\"libNCo\"] = mm.Courtyards(blg, spatial_weights=blg_q1).series\n",
    "blg[\"ldbPWL\"] = mm.PerimeterWall(blg, blg_q1).series\n",
    " \n",
    "blocks[\"ldkAre\"] = mm.Area(blocks).series\n",
    "blocks[\"ldkPer\"] = mm.Perimeter(blocks).series\n",
    "blocks[\"lskCCo\"] = mm.CircularCompactness(blocks, \"ldkAre\").series\n",
    "blocks[\"lskERI\"] = mm.EquivalentRectangularIndex(blocks, \"ldkAre\", \"ldkPer\").series\n",
    "blocks[\"lskCWA\"] = mm.CompactnessWeightedAxis(blocks, \"ldkAre\", \"ldkPer\").series\n",
    "blocks[\"ltkOri\"] = mm.Orientation(blocks).series\n",
    " \n",
    "blo_q1 = libpysal.weights.contiguity.Queen.from_dataframe(blocks, ids=\"bID\")\n",
    " \n",
    "blocks[\"ltkWNB\"] = mm.Neighbors(blocks, blo_q1, \"bID\", weighted=True).series\n",
    "blocks[\"likWBB\"] = mm.Count(blocks, blg, \"bID\", \"bID\", weighted=True).series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to parquets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess.drop(columns='geometry').to_parquet('files/tess_data.parquet')\n",
    "blg.drop(columns='geometry').to_parquet('files/blg_data.parquet')\n",
    "blocks.drop(columns='geometry').to_parquet('files/blocks_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen3 = mm.sw_high(k=3, weights=queen_1)\n",
    "queen1 = queen_1\n",
    "blg_queen = blg_q1\n",
    "\n",
    "blg['ltbIBD'] = mm.MeanInterbuildingDistance(blg, queen1, 'uID', queen3).series\n",
    "blg['ltcBuA'] = mm.BuildingAdjacency(blg, queen3, 'uID', blg_queen).series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess = tess.merge(blg[['floor_area', 'uID']], on='uID', how='left')\n",
    "tess['licGDe'] = mm.Density(tess, 'floor_area', queen3, 'uID', 'sdcAre').series\n",
    "tess = tess.drop(columns='floor_area')\n",
    "tess['ltcWRB'] = mm.BlocksCount(tess, 'bID', queen3, 'uID').series\n",
    "tess['sicCAR'] = mm.AreaRatio(tess, blg, 'sdcAre', 'sdbAre', 'uID').series\n",
    "tess['sicFAR'] = mm.AreaRatio(tess, blg, 'sdcAre', 'floor_area', 'uID').series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data to parquets and spatial weights matrices to gal files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess.drop(columns='geometry').to_parquet('files/tess_data.parquet')\n",
    "blg.drop(columns='geometry').to_parquet('files/blg_data.parquet')\n",
    " \n",
    "fo = libpysal.io.open('files/AMSqueen1.gal', 'w')\n",
    "fo.write(queen1)\n",
    "fo.close()\n",
    " \n",
    "fo = libpysal.io.open('files/AMSqueen3.gal', 'w')\n",
    "fo.write(queen3)\n",
    "fo.close()\n",
    " \n",
    "fo = libpysal.io.open('files/AMSblg_queen.gal', 'w')\n",
    "fo.write(blg_queen)\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets[\"sdsLen\"] = mm.Perimeter(streets).series\n",
    "tess[\"stcSAl\"] = mm.StreetAlignment(tess, streets, \"stcOri\", \"nID\").series\n",
    "blg[\"stbSAl\"] = mm.StreetAlignment(blg, streets, \"stbOri\", \"nID\").series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = mm.StreetProfile(streets, blg, heights='sdbHei', distance=3)\n",
    "streets[\"sdsSPW\"] = profile.w\n",
    "streets[\"sdsSPH\"] = profile.h\n",
    "streets[\"sdsSPR\"] = profile.p\n",
    "streets[\"sdsSPO\"] = profile.o\n",
    "streets[\"sdsSWD\"] = profile.wd\n",
    "streets[\"sdsSHD\"] = profile.hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streets[\"sssLin\"] = mm.Linearity(streets).series\n",
    "streets[\"sdsAre\"] = mm.Reached(streets, tess, \"nID\", \"nID\", mode=\"sum\", values=\"sdcAre\").series\n",
    "streets[\"sisBpM\"] = mm.Count(streets, blg, \"nID\", \"nID\", weighted=True).series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess.drop(columns='geometry').to_parquet('files/tess_data.parquet')\n",
    "blg.drop(columns='geometry').to_parquet('files/blg_data.parquet')\n",
    "streets.drop(columns='geometry').to_parquet('files/streets_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_q1 = libpysal.weights.contiguity.Queen.from_dataframe(streets)\n",
    " \n",
    "streets[\"misRea\"] = mm.Reached(\n",
    "    streets, tess, \"nID\", \"nID\", spatial_weights=str_q1, mode=\"count\"\n",
    ").series\n",
    "streets[\"mdsAre\"] = mm.Reached(streets, tess, \"nID\", \"nID\", spatial_weights=str_q1,\n",
    "                               mode=\"sum\").series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = mm.gdf_to_nx(streets)\n",
    " \n",
    "print(\"node degree\")\n",
    "graph = mm.node_degree(graph)\n",
    " \n",
    "print(\"subgraph\")\n",
    "graph = mm.subgraph(\n",
    "    graph,\n",
    "    radius=5,\n",
    "    meshedness=True,\n",
    "    cds_length=False,\n",
    "    mode=\"sum\",\n",
    "    degree=\"degree\",\n",
    "    length=\"mm_len\",\n",
    "    mean_node_degree=False,\n",
    "    proportion={0: True, 3: True, 4: True},\n",
    "    cyclomatic=False,\n",
    "    edge_node_ratio=False,\n",
    "    gamma=False,\n",
    "    local_closeness=True,\n",
    "    closeness_weight=\"mm_len\",\n",
    ")\n",
    "print(\"cds length\")\n",
    "graph = mm.cds_length(graph, radius=3, name=\"ldsCDL\")\n",
    " \n",
    "print(\"clustering\")\n",
    "graph = mm.clustering(graph, name=\"xcnSCl\")\n",
    " \n",
    "print(\"mean_node_dist\")\n",
    "graph = mm.mean_node_dist(graph, name=\"mtdMDi\")\n",
    " \n",
    "nodes, edges, sw = mm.nx_to_gdf(graph, spatial_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.to_file(path, layer=\"nodes\", driver=\"GPKG\")\n",
    "edges.to_file(path, layer=\"edges\", driver=\"GPKG\")\n",
    " \n",
    "fo = libpysal.io.open(\"files/nodes.gal\", \"w\")\n",
    "fo.write(sw)\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_w3 = mm.sw_high(k=3, gdf=edges)\n",
    "edges[\"ldsMSL\"] = mm.SegmentsLength(edges, spatial_weights=edges_w3, mean=True).series\n",
    " \n",
    "edges[\"ldsRea\"] = mm.Reached(edges, tess, \"nID\", \"nID\", spatial_weights=edges_w3).series\n",
    "edges[\"ldsRea\"] = mm.Reached(\n",
    "    edges, tess, \"nID\", \"nID\", spatial_weights=edges_w3, mode=\"sum\", values=\"sdcAre\"\n",
    ").series\n",
    " \n",
    "nodes_w5 = mm.sw_high(k=5, weights=sw)\n",
    "nodes[\"lddNDe\"] = mm.NodeDensity(nodes, edges, nodes_w5).series\n",
    "nodes[\"linWID\"] = mm.NodeDensity(\n",
    "    nodes, edges, nodes_w5, weighted=True, node_degree=\"degree\"\n",
    ").series\n",
    " \n",
    "blg[\"nodeID\"] = mm.get_node_id(blg, nodes, edges, \"nodeID\", \"nID\")\n",
    "tess = tess.merge(blg[[\"uID\", \"nodeID\"]], on=\"uID\", how=\"left\")\n",
    " \n",
    "nodes_w3 = mm.sw_high(k=3, weights=sw)\n",
    " \n",
    "nodes[\"lddRea\"] = mm.Reached(nodes, tess, \"nodeID\", \"nodeID\", nodes_w3).series\n",
    "nodes[\"lddARe\"] = mm.Reached(\n",
    "    nodes, tess, \"nodeID\", \"nodeID\", nodes_w3, mode=\"sum\", values=\"sdcAre\"\n",
    ").series\n",
    " \n",
    "nodes[\"sddAre\"] = mm.Reached(\n",
    "    nodes, tess, \"nodeID\", \"nodeID\", mode=\"sum\", values=\"sdcAre\"\n",
    ").series\n",
    "nodes[\"midRea\"] = mm.Reached(nodes, tess, \"nodeID\", \"nodeID\", spatial_weights=sw).series\n",
    "nodes[\"midAre\"] = mm.Reached(\n",
    "    nodes, tess, \"nodeID\", \"nodeID\", spatial_weights=sw, mode=\"sum\", values=\"sdcAre\"\n",
    ").series\n",
    " \n",
    "nodes.rename(\n",
    "    columns={\n",
    "        \"degree\": \"mtdDeg\",\n",
    "        \"meshedness\": \"lcdMes\",\n",
    "        \"local_closeness\": \"lcnClo\",\n",
    "        \"proportion_3\": \"linP3W\",\n",
    "        \"proportion_4\": \"linP4W\",\n",
    "        \"proportion_0\": \"linPDE\",\n",
    "    }, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess.drop(columns='geometry').to_parquet('files/tess_data.parquet')\n",
    "blg.drop(columns='geometry').to_parquet('files/blg_data.parquet')\n",
    "nodes.drop(columns='geometry').to_parquet('files/nodes_data.parquet')\n",
    "edges.drop(columns='geometry').to_parquet('files/edges_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tess.merge(blg.drop(columns=['nID', 'bID', 'nodeID', 'geometry']), on='uID')\n",
    "merged = merged.merge(blocks.drop(columns='geometry'), on='bID', how='left')\n",
    "merged = merged.merge(edges.drop(columns='geometry'), on='nID', how='left')\n",
    "merged = merged.merge(nodes.drop(columns='geometry'), on='nodeID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean columns to keep only measured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary = merged.drop(columns=['nID', 'bID', 'nodeID', 'mm_len', 'cdsbool', \n",
    "                               'node_start', 'node_end', 'geometry', 'floor_area'\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary.to_parquet('files/primary.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure contextual - spatially lagged characters\n",
    "\n",
    "This part measures contextual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = primary.set_index('uID')\n",
    "spatial_weights = queen3\n",
    "unique_id = 'uID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {}\n",
    "ranges = {}\n",
    "theils = {}\n",
    "\n",
    "for ch in gdf.columns:\n",
    "    means[ch] = []\n",
    "    ranges[ch] = []\n",
    "    theils[ch] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resolve potential missingness cause by invalid input data. That was not case in the presented case studies but may be case in subsequent research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.replace(np.inf, np.nan)  # normally does not happen, but to be sure\n",
    "gdf = gdf.fillna(0)  # normally does not happen, but to be sure\n",
    "chars = gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['lcdMes'] = gdf.apply(\n",
    "            lambda row: row.lcdMes if row.lcdMes >= 0 else 0,\n",
    "            axis=1,\n",
    "        )  # normally does not happen, but to be sure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Theil and Simpson functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theil(y):\n",
    "    y = np.array(y)\n",
    "    n = len(y)\n",
    "    plus = y + np.finfo('float').tiny * (y == 0)  # can't have 0 values\n",
    "    yt = plus.sum(axis=0)\n",
    "    s = plus / (yt * 1.0)\n",
    "    lns = np.log(n * s)\n",
    "    slns = s * lns\n",
    "    t = sum(slns)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simpson_di(data):\n",
    "\n",
    "    def p(n, N):\n",
    "        if n == 0:\n",
    "            return 0\n",
    "        return float(n) / N\n",
    "\n",
    "    N = sum(data.values())\n",
    "\n",
    "    return sum(p(n, N) ** 2 for n in data.values() if n != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over DataFrame and measure IQM, IQR and IDT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in tqdm(range(len(gdf)), total=gdf.shape[0]):\n",
    "    neighbours = [index]\n",
    "    neighbours += spatial_weights.neighbors[index]\n",
    "    \n",
    "    subset = gdf.iloc[neighbours]\n",
    "    for ch in chars:\n",
    "        values_list = subset[ch] \n",
    "        idec = limit_range(values_list, rng=(10, 90))\n",
    "        iquar = limit_range(values_list, rng=(25, 75))\n",
    "        \n",
    "        means[ch].append(np.mean(iquar))\n",
    "        ranges[ch].append(max(iquar) - min(iquar))\n",
    "        theils[ch].append(theil(idec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual = pd.DataFrame(index=gdf.index)\n",
    "for ch in chars:\n",
    "    contextual[ch + '_meanIQ3'] = means[ch]\n",
    "    contextual[ch + '_rangeIQ3'] = ranges[ch]\n",
    "    contextual[ch + '_theilID3'] = theils[ch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we measure Simpson's diversity.\n",
    "\n",
    "Skewness is used as an estimation of the distribution. Extremely skewed use HeadTail breaks for Simpson's binning, other Natural Breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness = pd.DataFrame(index=chars)\n",
    "for c in chars:\n",
    "    skewness.loc[c, 'skewness'] = sp.stats.skew(gdf[c])\n",
    "headtail = list(skewness.loc[skewness.skewness >= 1].index)\n",
    "to_invert = list(skewness.loc[skewness.skewness <= -1].index)\n",
    "\n",
    "for inv in to_invert:\n",
    "    gdf[inv] = gdf[inv].max() - gdf[inv]\n",
    "headtail = headtail + to_invert\n",
    "natural = [c for c in chars if c not in headtail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(natural) + len(headtail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create global bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for c in headtail + natural:\n",
    "    results[c] = []\n",
    "bins = {}\n",
    "for c in headtail:\n",
    "    bins[c] = mapclassify.HeadTailBreaks(gdf[c]).bins\n",
    "for c in natural:\n",
    "    bins[c] = mapclassify.gadf(gdf[c], method='NaturalBreaks')[1].bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And measure local Simpson's diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in tqdm(gdf.index, total=gdf.shape[0]):\n",
    "    neighbours = [index]\n",
    "    neighbours += spatial_weights.neighbors[index]\n",
    "    \n",
    "    subset = gdf.loc[neighbours]\n",
    "    for c in headtail + natural:\n",
    "        values = subset[c]\n",
    "        sample_bins = mapclassify.UserDefined(values, list(bins[c]))\n",
    "        counts = dict(zip(bins[c], sample_bins.counts))\n",
    "        results[c].append(_simpson_di(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in headtail + natural:\n",
    "    contextual[c + '_simpson'] = results[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual.to_parquet('files/contextual.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "We use contextual characters to do GMM clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = contextual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we standardize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise data\n",
    "\n",
    "x = data.values\n",
    "scaler = preprocessing.StandardScaler()\n",
    "cols = list(data.columns)\n",
    "data[cols] = scaler.fit_transform(data[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure BIC to estimate optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic = pd.DataFrame(columns=['n', 'bic', 'run'])\n",
    "ix = 0\n",
    "\n",
    "n_components_range = range(2, 6) # specify range you want to assess\n",
    "gmmruns = 3  # specify how many times should each option be tried (more better, but takes a long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(0)\n",
    "for n_components in n_components_range:\n",
    "    for i in range(gmmruns):\n",
    "        gmm = GaussianMixture(n_components=n_components, covariance_type=\"full\", max_iter=200, n_init=1, verbose=1)\n",
    "        fitted = gmm.fit(data)\n",
    "        bicnum = gmm.bic(data)\n",
    "        bic.loc[ix] = [n_components, bicnum, i]\n",
    "        ix += 1\n",
    "\n",
    "        print(n_components, i, \"BIC:\", bicnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic.to_csv('files/complete_BIC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot below, we estimate the optimal `n` as the first significant minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "sns.lineplot(ax=ax, x='n', y='bic', data=bic)\n",
    "# plt.savefig('files/complete_BIC.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4  # based on above\n",
    "n_init = 5  # more initialization, more stable clustering gets\n",
    "\n",
    "gmm = GaussianMixture(n_components=n, covariance_type=\"full\", max_iter=200, n_init=n_init, verbose=1)\n",
    "fitted = gmm.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cluster'] = gmm.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index()[['cluster', 'uID']].to_csv('files/cluster_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = data.reset_index()[['cluster', 'uID']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierachical clustering\n",
    "\n",
    "Finally, we create hierarchical classification - taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = data.groupby('cluster').mean()\n",
    "Z = hierarchy.linkage(group, 'ward')\n",
    "plt.figure(figsize=(25, 10))\n",
    "dn = hierarchy.dendrogram(Z, labels=group.index)\n",
    "\n",
    "# plt.savefig('tree.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}